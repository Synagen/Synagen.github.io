[
  {
    "objectID": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html",
    "href": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html",
    "title": "Who Directed That?",
    "section": "",
    "text": "Starting off the Synagen blog with a fun one. This is really just a small toy example working through the FastAI practical deep learning course, specifically Lesson 1 and Lesson 2 content.\nIn this post I show (interactively) how easy it is to train a simple object detection neural net in just a few lines of python code.\nDeep Learning has made some serious advances in recent years when it comes to image and video data, so it shouldn’t surprise anyone how well architectures like Convolutional Neural Networks (CNNs) can easily identify humans/animals/objects in photos.\nSo instead of doing yet another dog/cat classifier, I set out to challenge the AI a bit: could it detect who directed a movie based only on a screenshot, and only after being trained on other movies from the same director?\nThat last bit is quite important - I assume it would be no challenge to detect scenes from the same movie that the model was trained on, but will the learnings about a director’s artistic style translate even to new movies that the model has never seen before?\nTo find out, I picked two of my favourite directors - Wes Anderson and David Lynch - , then downloaded a bunch of images of scenes from some of their most iconic works, and trained a deep learning model to classify each scene.\nRead on to find out how it went…\n\n\n\nfrom fastbook import *\nfrom fastai.vision import *\nfrom fastai.vision.widgets import *\nfrom decouple import config, AutoConfig\n\n\n\n\nHere we use Bing to search for images. I found that adding the director’s name and ‘scene’ at the end of the search query yielded best results. Below is a sample image from Anderson’s earliest film: Bottle Rocket\n\nconfig = AutoConfig(' ')\nkey = config('AZURE_SEARCH_KEY')\n\n\nresults = search_images_bing(key, 'wes anderson bottle rocket scene')\nims = results.attrgot('contentUrl')\nlen(ims)\n\n150\n\n\n\ndest = 'bottlerocket.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      100.09% [2531328/2529007 00:02&lt;00:00]\n    \n    \n\n\nPath('bottlerocket.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(512,512)\n\n\n\n\nLet’s get the rest of the movie scenes downloaded!\nHere I separate our data into training and validation sets. Specifically, I picked The French Dispatch and Moonrise Kingdom as Wes Anderson validation movies, and similarly singled out Mulholland Drive and Blue Velvet from Lynch’s colletion.\nI did find that if we downloaded too many images from each movie, they would start getting repetitive/not relevant (e.g. pictures of movie posters rather than actual scenes from the movies), so I limit the search to the first 30 results from each.\n\nwa_movies_train = ['bottle rocket','rushmore','the royal tenebaums', 'the life aquatic with steve zissou',\n            'the darjeeling limited','the grand budapest hotel','asteroid city']\nwa_movies_val = ['the french dispatch','moonrise kingdom',]\n\ndl_movies_train = ['eraserhead','the elephant man','wild at heart',\n            'twin peaks','lost highway','inland empire']\ndl_movies_val = ['mulholland drive','blue velvet',]\n\npath = Path('movies')\n\n\nif not path.exists():\n    path.mkdir()\ntrain_path = (path/'training')\nif not train_path.exists():\n    train_path.mkdir()\n    for o in wa_movies_train:\n        dest = (train_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_train:\n        dest = (train_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\n\nval_path = (path/'validation')\nif not val_path.exists():\n    val_path.mkdir()\n    for o in wa_movies_val:\n        dest = (val_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_val:\n        dest = (val_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\nHere we remove any broken links:\n\nfns = get_image_files(path)\nfailed = verify_images(fns)\nfailed.map(Path.unlink);\n\n\n\n\nNow that we have all our data collected, let’s prepare it for our ML model using fastai’s DataBlock class:\nTo break it down, we specify the following: - The type of blocks (input = images, output = categories) - How to get our data? Using the get_image_files function - How to split training/validation? Based on the folder names 2 levels up - How to label our data? Based on the folder names 1 level up - How to transform our data? Resize all images to a standard 256x256 pixels\n\nscenes = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=GrandparentSplitter(train_name='training', valid_name='validation'),\n    get_y=parent_label,\n    item_tfms=Resize(256))\n\nNow that we have everything specified, let’s load in our data (making sure to suffle both train and val sets) and display a batch of images:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\n\n\ndl_train.train.show_batch(max_n=8, nrows=2)\n\n\n\n\nLooking good…\nThere may be some images in here that we want to remove (e.g. ones with movie titles, or multiple scenes in one image), but we will deal with that later.\nFirst, let’s actually train our model and see how it does!\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(6)\n\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.082164\n0.818760\n0.500000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.526650\n0.399322\n0.500000\n00:10\n\n\n1\n0.397128\n0.348378\n0.500000\n00:10\n\n\n2\n0.327281\n0.306815\n0.500000\n00:10\n\n\n3\n0.260668\n0.295519\n0.500000\n00:10\n\n\n4\n0.215509\n0.300533\n0.500000\n00:10\n\n\n5\n0.190804\n0.298368\n0.500000\n00:10\n\n\n\n\n\nSomething interesting here is that even though we trained for 6 epochs, the valiation loss actually stopped improving after only 3. We could have stopped training there - the training loss still improves but this is likely just overfitting on our input data.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo that took about a minute to train*, and we now have a machine learning model that can tell the difference between Wes Anderson and David Lynch in over 95% of our examples.\n*we did use a pretrained model so that saved a LOT of time!\nLet’s now go through our dataset and remove any shots of filming, duplicates, or other images that are just adding noise.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nNow let’s run our data prep -&gt; model training -&gt; accuracy scoring steps again:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\ndl_train.valid.show_batch(max_n=8, nrows=2)\n\n\n\n\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.234754\n1.198495\n0.490000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.517711\n0.332291\n0.490000\n00:10\n\n\n1\n0.383631\n0.248246\n0.490000\n00:10\n\n\n2\n0.298077\n0.262733\n0.490000\n00:10\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd what do the results actually look like? Below we take a look at some validation set images, with the actual and predicted labels printed above the pictures. Not bad!\n\ninterp.show_results(list(range(0,100,4)))\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at which images the model struggles with by using plot_top_losses. I’d say that even diehard fans might struggle with a couple of these!\n\ninterp.plot_top_losses(6, nrows=2)\n\n\n\n\n\n\n\n\n\n\n\nThe first shot of Saoirse Ronan from The French Dispatch looks a lot like Naomi Watts in Mulholland Drive, while the middle lower image of Timothée Chalamet could have been taken straight out of Lynch’s Eraserhead!\n\n#hide\nf, axarr = plt.subplots(2,2)\nf.set_figheight(8)\nf.set_figwidth(12)\naxarr[0,0].imshow(Image.open(Path('movies')/'validation/wesanderson/20fa22db-b9d3-4eae-9537-d4c2c68095c4.jpg'))\naxarr[0,0].set_title(\"The French Dispatch\")\naxarr[0,0].axis('off')\naxarr[0,1].imshow(Image.open(Path('movies')/'validation/davidlynch/b8f11185-392e-4be4-a2d6-99b00cb04783.jpg'))\naxarr[0,1].set_title(\"Mulholland Drive\")\naxarr[0,1].axis('off')\naxarr[1,0].imshow(Image.open(Path('movies')/'validation/wesanderson/3e787a80-3f2e-4dc5-a27d-82ea56733a15.jpg'))\naxarr[1,0].set_title(\"The French Dispatch\")\naxarr[1,0].axis('off')\naxarr[1,1].imshow(Image.open(Path('movies')/'training/davidlynch/2e17a81a-c911-4f23-afe5-5c89bdc07bdf.png'))\naxarr[1,1].set_title(\"Eraserhead\")\naxarr[1,1].axis('off')\n\n(-0.5, 1279.5, 799.5, -0.5)\n\n\n\n\n\nCoincidence? Or perhaps some artistic influence from Lynch’s earlier works infiltrated Anderson’s mind when producing The French Dispatch? Well if even our state-of-the-art ML model cannot tell them apart, perhaps there is some evidence of inspiration, but I guess we’ll never know for sure…"
  },
  {
    "objectID": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html#a-ml-model-to-detect-whether-thats-a-wes-anderson-or-a-david-lynch-movie",
    "href": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html#a-ml-model-to-detect-whether-thats-a-wes-anderson-or-a-david-lynch-movie",
    "title": "Who Directed That?",
    "section": "",
    "text": "Starting off the Synagen blog with a fun one. This is really just a small toy example working through the FastAI practical deep learning course, specifically Lesson 1 and Lesson 2 content.\nIn this post I show (interactively) how easy it is to train a simple object detection neural net in just a few lines of python code.\nDeep Learning has made some serious advances in recent years when it comes to image and video data, so it shouldn’t surprise anyone how well architectures like Convolutional Neural Networks (CNNs) can easily identify humans/animals/objects in photos.\nSo instead of doing yet another dog/cat classifier, I set out to challenge the AI a bit: could it detect who directed a movie based only on a screenshot, and only after being trained on other movies from the same director?\nThat last bit is quite important - I assume it would be no challenge to detect scenes from the same movie that the model was trained on, but will the learnings about a director’s artistic style translate even to new movies that the model has never seen before?\nTo find out, I picked two of my favourite directors - Wes Anderson and David Lynch - , then downloaded a bunch of images of scenes from some of their most iconic works, and trained a deep learning model to classify each scene.\nRead on to find out how it went…\n\n\n\nfrom fastbook import *\nfrom fastai.vision import *\nfrom fastai.vision.widgets import *\nfrom decouple import config, AutoConfig\n\n\n\n\nHere we use Bing to search for images. I found that adding the director’s name and ‘scene’ at the end of the search query yielded best results. Below is a sample image from Anderson’s earliest film: Bottle Rocket\n\nconfig = AutoConfig(' ')\nkey = config('AZURE_SEARCH_KEY')\n\n\nresults = search_images_bing(key, 'wes anderson bottle rocket scene')\nims = results.attrgot('contentUrl')\nlen(ims)\n\n150\n\n\n\ndest = 'bottlerocket.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      100.09% [2531328/2529007 00:02&lt;00:00]\n    \n    \n\n\nPath('bottlerocket.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(512,512)\n\n\n\n\nLet’s get the rest of the movie scenes downloaded!\nHere I separate our data into training and validation sets. Specifically, I picked The French Dispatch and Moonrise Kingdom as Wes Anderson validation movies, and similarly singled out Mulholland Drive and Blue Velvet from Lynch’s colletion.\nI did find that if we downloaded too many images from each movie, they would start getting repetitive/not relevant (e.g. pictures of movie posters rather than actual scenes from the movies), so I limit the search to the first 30 results from each.\n\nwa_movies_train = ['bottle rocket','rushmore','the royal tenebaums', 'the life aquatic with steve zissou',\n            'the darjeeling limited','the grand budapest hotel','asteroid city']\nwa_movies_val = ['the french dispatch','moonrise kingdom',]\n\ndl_movies_train = ['eraserhead','the elephant man','wild at heart',\n            'twin peaks','lost highway','inland empire']\ndl_movies_val = ['mulholland drive','blue velvet',]\n\npath = Path('movies')\n\n\nif not path.exists():\n    path.mkdir()\ntrain_path = (path/'training')\nif not train_path.exists():\n    train_path.mkdir()\n    for o in wa_movies_train:\n        dest = (train_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_train:\n        dest = (train_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\n\nval_path = (path/'validation')\nif not val_path.exists():\n    val_path.mkdir()\n    for o in wa_movies_val:\n        dest = (val_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_val:\n        dest = (val_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\nHere we remove any broken links:\n\nfns = get_image_files(path)\nfailed = verify_images(fns)\nfailed.map(Path.unlink);\n\n\n\n\nNow that we have all our data collected, let’s prepare it for our ML model using fastai’s DataBlock class:\nTo break it down, we specify the following: - The type of blocks (input = images, output = categories) - How to get our data? Using the get_image_files function - How to split training/validation? Based on the folder names 2 levels up - How to label our data? Based on the folder names 1 level up - How to transform our data? Resize all images to a standard 256x256 pixels\n\nscenes = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=GrandparentSplitter(train_name='training', valid_name='validation'),\n    get_y=parent_label,\n    item_tfms=Resize(256))\n\nNow that we have everything specified, let’s load in our data (making sure to suffle both train and val sets) and display a batch of images:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\n\n\ndl_train.train.show_batch(max_n=8, nrows=2)\n\n\n\n\nLooking good…\nThere may be some images in here that we want to remove (e.g. ones with movie titles, or multiple scenes in one image), but we will deal with that later.\nFirst, let’s actually train our model and see how it does!\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(6)\n\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.082164\n0.818760\n0.500000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.526650\n0.399322\n0.500000\n00:10\n\n\n1\n0.397128\n0.348378\n0.500000\n00:10\n\n\n2\n0.327281\n0.306815\n0.500000\n00:10\n\n\n3\n0.260668\n0.295519\n0.500000\n00:10\n\n\n4\n0.215509\n0.300533\n0.500000\n00:10\n\n\n5\n0.190804\n0.298368\n0.500000\n00:10\n\n\n\n\n\nSomething interesting here is that even though we trained for 6 epochs, the valiation loss actually stopped improving after only 3. We could have stopped training there - the training loss still improves but this is likely just overfitting on our input data.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo that took about a minute to train*, and we now have a machine learning model that can tell the difference between Wes Anderson and David Lynch in over 95% of our examples.\n*we did use a pretrained model so that saved a LOT of time!\nLet’s now go through our dataset and remove any shots of filming, duplicates, or other images that are just adding noise.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nNow let’s run our data prep -&gt; model training -&gt; accuracy scoring steps again:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\ndl_train.valid.show_batch(max_n=8, nrows=2)\n\n\n\n\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.234754\n1.198495\n0.490000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.517711\n0.332291\n0.490000\n00:10\n\n\n1\n0.383631\n0.248246\n0.490000\n00:10\n\n\n2\n0.298077\n0.262733\n0.490000\n00:10\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd what do the results actually look like? Below we take a look at some validation set images, with the actual and predicted labels printed above the pictures. Not bad!\n\ninterp.show_results(list(range(0,100,4)))\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at which images the model struggles with by using plot_top_losses. I’d say that even diehard fans might struggle with a couple of these!\n\ninterp.plot_top_losses(6, nrows=2)\n\n\n\n\n\n\n\n\n\n\n\nThe first shot of Saoirse Ronan from The French Dispatch looks a lot like Naomi Watts in Mulholland Drive, while the middle lower image of Timothée Chalamet could have been taken straight out of Lynch’s Eraserhead!\n\n#hide\nf, axarr = plt.subplots(2,2)\nf.set_figheight(8)\nf.set_figwidth(12)\naxarr[0,0].imshow(Image.open(Path('movies')/'validation/wesanderson/20fa22db-b9d3-4eae-9537-d4c2c68095c4.jpg'))\naxarr[0,0].set_title(\"The French Dispatch\")\naxarr[0,0].axis('off')\naxarr[0,1].imshow(Image.open(Path('movies')/'validation/davidlynch/b8f11185-392e-4be4-a2d6-99b00cb04783.jpg'))\naxarr[0,1].set_title(\"Mulholland Drive\")\naxarr[0,1].axis('off')\naxarr[1,0].imshow(Image.open(Path('movies')/'validation/wesanderson/3e787a80-3f2e-4dc5-a27d-82ea56733a15.jpg'))\naxarr[1,0].set_title(\"The French Dispatch\")\naxarr[1,0].axis('off')\naxarr[1,1].imshow(Image.open(Path('movies')/'training/davidlynch/2e17a81a-c911-4f23-afe5-5c89bdc07bdf.png'))\naxarr[1,1].set_title(\"Eraserhead\")\naxarr[1,1].axis('off')\n\n(-0.5, 1279.5, 799.5, -0.5)\n\n\n\n\n\nCoincidence? Or perhaps some artistic influence from Lynch’s earlier works infiltrated Anderson’s mind when producing The French Dispatch? Well if even our state-of-the-art ML model cannot tell them apart, perhaps there is some evidence of inspiration, but I guess we’ll never know for sure…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Blog about Artificial Intelligence",
    "section": "",
    "text": "Who Directed That?\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\ndeep learning\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nAgentRichi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]