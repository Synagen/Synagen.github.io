[
  {
    "objectID": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html",
    "href": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html",
    "title": "Who Directed That?",
    "section": "",
    "text": "Starting off the Synagen blog with a fun one. This is really just a small toy example working through the FastAI practical deep learning course, specifically Lesson 1 and Lesson 2 content.\nIn this post I show (interactively) how easy it is to train a simple object detection neural net in just a few lines of python code.\nDeep Learning has made some serious advances in recent years when it comes to image and video data, so it shouldn’t surprise anyone how well architectures like Convolutional Neural Networks (CNNs) can easily identify humans/animals/objects in photos.\nSo instead of doing yet another dog/cat classifier, I set out to challenge the AI a bit: could it detect who directed a movie based only on a screenshot, and only after being trained on other movies from the same director?\nThat last bit is quite important - I assume it would be no challenge to detect scenes from the same movie that the model was trained on, but will the learnings about a director’s artistic style translate even to new movies that the model has never seen before?\nTo find out, I picked two of my favourite directors - Wes Anderson and David Lynch - , then downloaded a bunch of images of scenes from some of their most iconic works, and trained a deep learning model to classify each scene.\nRead on to find out how it went…\n\n\n\nfrom fastbook import *\nfrom fastai.vision import *\nfrom fastai.vision.widgets import *\nfrom decouple import config, AutoConfig\n\n\n\n\nHere we use Bing to search for images. I found that adding the director’s name and ‘scene’ at the end of the search query yielded best results. Below is a sample image from Anderson’s earliest film: Bottle Rocket\n\nconfig = AutoConfig(' ')\nkey = config('AZURE_SEARCH_KEY')\n\n\nresults = search_images_bing(key, 'wes anderson bottle rocket scene')\nims = results.attrgot('contentUrl')\nlen(ims)\n\n150\n\n\n\ndest = 'bottlerocket.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      100.09% [2531328/2529007 00:02&lt;00:00]\n    \n    \n\n\nPath('bottlerocket.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(512,512)\n\n\n\n\n\n\n\n\nLet’s get the rest of the movie scenes downloaded!\nHere I separate our data into training and validation sets. Specifically, I picked The French Dispatch and Moonrise Kingdom as Wes Anderson validation movies, and similarly singled out Mulholland Drive and Blue Velvet from Lynch’s colletion.\nI did find that if we downloaded too many images from each movie, they would start getting repetitive/not relevant (e.g. pictures of movie posters rather than actual scenes from the movies), so I limit the search to the first 30 results from each.\n\nwa_movies_train = ['bottle rocket','rushmore','the royal tenebaums', 'the life aquatic with steve zissou',\n            'the darjeeling limited','the grand budapest hotel','asteroid city']\nwa_movies_val = ['the french dispatch','moonrise kingdom',]\n\ndl_movies_train = ['eraserhead','the elephant man','wild at heart',\n            'twin peaks','lost highway','inland empire']\ndl_movies_val = ['mulholland drive','blue velvet',]\n\npath = Path('movies')\n\n\nif not path.exists():\n    path.mkdir()\ntrain_path = (path/'training')\nif not train_path.exists():\n    train_path.mkdir()\n    for o in wa_movies_train:\n        dest = (train_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_train:\n        dest = (train_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\n\nval_path = (path/'validation')\nif not val_path.exists():\n    val_path.mkdir()\n    for o in wa_movies_val:\n        dest = (val_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_val:\n        dest = (val_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\nHere we remove any broken links:\n\nfns = get_image_files(path)\nfailed = verify_images(fns)\nfailed.map(Path.unlink);\n\n\n\n\nNow that we have all our data collected, let’s prepare it for our ML model using fastai’s DataBlock class:\nTo break it down, we specify the following: - The type of blocks (input = images, output = categories) - How to get our data? Using the get_image_files function - How to split training/validation? Based on the folder names 2 levels up - How to label our data? Based on the folder names 1 level up - How to transform our data? Resize all images to a standard 256x256 pixels\n\nscenes = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=GrandparentSplitter(train_name='training', valid_name='validation'),\n    get_y=parent_label,\n    item_tfms=Resize(256))\n\nNow that we have everything specified, let’s load in our data (making sure to suffle both train and val sets) and display a batch of images:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\n\n\ndl_train.train.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\nLooking good…\nThere may be some images in here that we want to remove (e.g. ones with movie titles, or multiple scenes in one image), but we will deal with that later.\nFirst, let’s actually train our model and see how it does!\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(6)\n\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.082164\n0.818760\n0.500000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.526650\n0.399322\n0.500000\n00:10\n\n\n1\n0.397128\n0.348378\n0.500000\n00:10\n\n\n2\n0.327281\n0.306815\n0.500000\n00:10\n\n\n3\n0.260668\n0.295519\n0.500000\n00:10\n\n\n4\n0.215509\n0.300533\n0.500000\n00:10\n\n\n5\n0.190804\n0.298368\n0.500000\n00:10\n\n\n\n\n\nSomething interesting here is that even though we trained for 6 epochs, the valiation loss actually stopped improving after only 3. We could have stopped training there - the training loss still improves but this is likely just overfitting on our input data.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo that took about a minute to train*, and we now have a machine learning model that can tell the difference between Wes Anderson and David Lynch in over 95% of our examples.\n*we did use a pretrained model so that saved a LOT of time!\nLet’s now go through our dataset and remove any shots of filming, duplicates, or other images that are just adding noise.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nNow let’s run our data prep -&gt; model training -&gt; accuracy scoring steps again:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\ndl_train.valid.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.234754\n1.198495\n0.490000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.517711\n0.332291\n0.490000\n00:10\n\n\n1\n0.383631\n0.248246\n0.490000\n00:10\n\n\n2\n0.298077\n0.262733\n0.490000\n00:10\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd what do the results actually look like? Below we take a look at some validation set images, with the actual and predicted labels printed above the pictures. Not bad!\n\ninterp.show_results(list(range(0,100,4)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at which images the model struggles with by using plot_top_losses. I’d say that even diehard fans might struggle with a couple of these!\n\ninterp.plot_top_losses(6, nrows=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first shot of Saoirse Ronan from The French Dispatch looks a lot like Naomi Watts in Mulholland Drive, while the middle lower image of Timothée Chalamet could have been taken straight out of Lynch’s Eraserhead!\n\n#hide\nf, axarr = plt.subplots(2,2)\nf.set_figheight(8)\nf.set_figwidth(12)\naxarr[0,0].imshow(Image.open(Path('movies')/'validation/wesanderson/20fa22db-b9d3-4eae-9537-d4c2c68095c4.jpg'))\naxarr[0,0].set_title(\"The French Dispatch\")\naxarr[0,0].axis('off')\naxarr[0,1].imshow(Image.open(Path('movies')/'validation/davidlynch/b8f11185-392e-4be4-a2d6-99b00cb04783.jpg'))\naxarr[0,1].set_title(\"Mulholland Drive\")\naxarr[0,1].axis('off')\naxarr[1,0].imshow(Image.open(Path('movies')/'validation/wesanderson/3e787a80-3f2e-4dc5-a27d-82ea56733a15.jpg'))\naxarr[1,0].set_title(\"The French Dispatch\")\naxarr[1,0].axis('off')\naxarr[1,1].imshow(Image.open(Path('movies')/'training/davidlynch/2e17a81a-c911-4f23-afe5-5c89bdc07bdf.png'))\naxarr[1,1].set_title(\"Eraserhead\")\naxarr[1,1].axis('off')\n\n\n\n\n\n\n\n\nCoincidence? Or perhaps some artistic influence from Lynch’s earlier works infiltrated Anderson’s mind when producing The French Dispatch? Well if even our state-of-the-art ML model cannot tell them apart, perhaps there is some evidence of inspiration, but I guess we’ll never know for sure…"
  },
  {
    "objectID": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html#a-ml-model-to-detect-whether-thats-a-wes-anderson-or-a-david-lynch-movie",
    "href": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html#a-ml-model-to-detect-whether-thats-a-wes-anderson-or-a-david-lynch-movie",
    "title": "Who Directed That?",
    "section": "",
    "text": "Starting off the Synagen blog with a fun one. This is really just a small toy example working through the FastAI practical deep learning course, specifically Lesson 1 and Lesson 2 content.\nIn this post I show (interactively) how easy it is to train a simple object detection neural net in just a few lines of python code.\nDeep Learning has made some serious advances in recent years when it comes to image and video data, so it shouldn’t surprise anyone how well architectures like Convolutional Neural Networks (CNNs) can easily identify humans/animals/objects in photos.\nSo instead of doing yet another dog/cat classifier, I set out to challenge the AI a bit: could it detect who directed a movie based only on a screenshot, and only after being trained on other movies from the same director?\nThat last bit is quite important - I assume it would be no challenge to detect scenes from the same movie that the model was trained on, but will the learnings about a director’s artistic style translate even to new movies that the model has never seen before?\nTo find out, I picked two of my favourite directors - Wes Anderson and David Lynch - , then downloaded a bunch of images of scenes from some of their most iconic works, and trained a deep learning model to classify each scene.\nRead on to find out how it went…\n\n\n\nfrom fastbook import *\nfrom fastai.vision import *\nfrom fastai.vision.widgets import *\nfrom decouple import config, AutoConfig\n\n\n\n\nHere we use Bing to search for images. I found that adding the director’s name and ‘scene’ at the end of the search query yielded best results. Below is a sample image from Anderson’s earliest film: Bottle Rocket\n\nconfig = AutoConfig(' ')\nkey = config('AZURE_SEARCH_KEY')\n\n\nresults = search_images_bing(key, 'wes anderson bottle rocket scene')\nims = results.attrgot('contentUrl')\nlen(ims)\n\n150\n\n\n\ndest = 'bottlerocket.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      100.09% [2531328/2529007 00:02&lt;00:00]\n    \n    \n\n\nPath('bottlerocket.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(512,512)\n\n\n\n\n\n\n\n\nLet’s get the rest of the movie scenes downloaded!\nHere I separate our data into training and validation sets. Specifically, I picked The French Dispatch and Moonrise Kingdom as Wes Anderson validation movies, and similarly singled out Mulholland Drive and Blue Velvet from Lynch’s colletion.\nI did find that if we downloaded too many images from each movie, they would start getting repetitive/not relevant (e.g. pictures of movie posters rather than actual scenes from the movies), so I limit the search to the first 30 results from each.\n\nwa_movies_train = ['bottle rocket','rushmore','the royal tenebaums', 'the life aquatic with steve zissou',\n            'the darjeeling limited','the grand budapest hotel','asteroid city']\nwa_movies_val = ['the french dispatch','moonrise kingdom',]\n\ndl_movies_train = ['eraserhead','the elephant man','wild at heart',\n            'twin peaks','lost highway','inland empire']\ndl_movies_val = ['mulholland drive','blue velvet',]\n\npath = Path('movies')\n\n\nif not path.exists():\n    path.mkdir()\ntrain_path = (path/'training')\nif not train_path.exists():\n    train_path.mkdir()\n    for o in wa_movies_train:\n        dest = (train_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_train:\n        dest = (train_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\n\nval_path = (path/'validation')\nif not val_path.exists():\n    val_path.mkdir()\n    for o in wa_movies_val:\n        dest = (val_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_val:\n        dest = (val_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\nHere we remove any broken links:\n\nfns = get_image_files(path)\nfailed = verify_images(fns)\nfailed.map(Path.unlink);\n\n\n\n\nNow that we have all our data collected, let’s prepare it for our ML model using fastai’s DataBlock class:\nTo break it down, we specify the following: - The type of blocks (input = images, output = categories) - How to get our data? Using the get_image_files function - How to split training/validation? Based on the folder names 2 levels up - How to label our data? Based on the folder names 1 level up - How to transform our data? Resize all images to a standard 256x256 pixels\n\nscenes = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=GrandparentSplitter(train_name='training', valid_name='validation'),\n    get_y=parent_label,\n    item_tfms=Resize(256))\n\nNow that we have everything specified, let’s load in our data (making sure to suffle both train and val sets) and display a batch of images:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\n\n\ndl_train.train.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\nLooking good…\nThere may be some images in here that we want to remove (e.g. ones with movie titles, or multiple scenes in one image), but we will deal with that later.\nFirst, let’s actually train our model and see how it does!\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(6)\n\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.082164\n0.818760\n0.500000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.526650\n0.399322\n0.500000\n00:10\n\n\n1\n0.397128\n0.348378\n0.500000\n00:10\n\n\n2\n0.327281\n0.306815\n0.500000\n00:10\n\n\n3\n0.260668\n0.295519\n0.500000\n00:10\n\n\n4\n0.215509\n0.300533\n0.500000\n00:10\n\n\n5\n0.190804\n0.298368\n0.500000\n00:10\n\n\n\n\n\nSomething interesting here is that even though we trained for 6 epochs, the valiation loss actually stopped improving after only 3. We could have stopped training there - the training loss still improves but this is likely just overfitting on our input data.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo that took about a minute to train*, and we now have a machine learning model that can tell the difference between Wes Anderson and David Lynch in over 95% of our examples.\n*we did use a pretrained model so that saved a LOT of time!\nLet’s now go through our dataset and remove any shots of filming, duplicates, or other images that are just adding noise.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nNow let’s run our data prep -&gt; model training -&gt; accuracy scoring steps again:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\ndl_train.valid.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.234754\n1.198495\n0.490000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.517711\n0.332291\n0.490000\n00:10\n\n\n1\n0.383631\n0.248246\n0.490000\n00:10\n\n\n2\n0.298077\n0.262733\n0.490000\n00:10\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd what do the results actually look like? Below we take a look at some validation set images, with the actual and predicted labels printed above the pictures. Not bad!\n\ninterp.show_results(list(range(0,100,4)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at which images the model struggles with by using plot_top_losses. I’d say that even diehard fans might struggle with a couple of these!\n\ninterp.plot_top_losses(6, nrows=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first shot of Saoirse Ronan from The French Dispatch looks a lot like Naomi Watts in Mulholland Drive, while the middle lower image of Timothée Chalamet could have been taken straight out of Lynch’s Eraserhead!\n\n#hide\nf, axarr = plt.subplots(2,2)\nf.set_figheight(8)\nf.set_figwidth(12)\naxarr[0,0].imshow(Image.open(Path('movies')/'validation/wesanderson/20fa22db-b9d3-4eae-9537-d4c2c68095c4.jpg'))\naxarr[0,0].set_title(\"The French Dispatch\")\naxarr[0,0].axis('off')\naxarr[0,1].imshow(Image.open(Path('movies')/'validation/davidlynch/b8f11185-392e-4be4-a2d6-99b00cb04783.jpg'))\naxarr[0,1].set_title(\"Mulholland Drive\")\naxarr[0,1].axis('off')\naxarr[1,0].imshow(Image.open(Path('movies')/'validation/wesanderson/3e787a80-3f2e-4dc5-a27d-82ea56733a15.jpg'))\naxarr[1,0].set_title(\"The French Dispatch\")\naxarr[1,0].axis('off')\naxarr[1,1].imshow(Image.open(Path('movies')/'training/davidlynch/2e17a81a-c911-4f23-afe5-5c89bdc07bdf.png'))\naxarr[1,1].set_title(\"Eraserhead\")\naxarr[1,1].axis('off')\n\n\n\n\n\n\n\n\nCoincidence? Or perhaps some artistic influence from Lynch’s earlier works infiltrated Anderson’s mind when producing The French Dispatch? Well if even our state-of-the-art ML model cannot tell them apart, perhaps there is some evidence of inspiration, but I guess we’ll never know for sure…"
  },
  {
    "objectID": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html",
    "href": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html",
    "title": "Forecasting Caravan Park Visitors",
    "section": "",
    "text": "This report provides a forecasting methodology for caravan park visitors, applied to tourism in New South Wales (NSW), Australia. The forecasts are based on historical data from the International Visitor Survey (IVS), National Visitor Survey (NVS), and Compass IoT connected vehicle trips. The forecast includes the average daily number of visitors per campsite for each month from 2029 to 2040. Numbers for 2025-2028 are available directly from Tourism Research Australia (TRA) and are not forecasted, however the per-caravan site forecast can also be provided for these years if required."
  },
  {
    "objectID": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#introduction",
    "href": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#introduction",
    "title": "Forecasting Caravan Park Visitors",
    "section": "",
    "text": "This report provides a forecasting methodology for caravan park visitors, applied to tourism in New South Wales (NSW), Australia. The forecasts are based on historical data from the International Visitor Survey (IVS), National Visitor Survey (NVS), and Compass IoT connected vehicle trips. The forecast includes the average daily number of visitors per campsite for each month from 2029 to 2040. Numbers for 2025-2028 are available directly from Tourism Research Australia (TRA) and are not forecasted, however the per-caravan site forecast can also be provided for these years if required."
  },
  {
    "objectID": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#overview-of-tourism-visitation-trends",
    "href": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#overview-of-tourism-visitation-trends",
    "title": "Forecasting Caravan Park Visitors",
    "section": "Overview of Tourism Visitation Trends",
    "text": "Overview of Tourism Visitation Trends\nTourism visitation patterns for caravan parks across the Hawkesbury-Nepean Valley (HNV) were assessed through a multi-source data integration approach. This included:\n\nTourism Research Australia (TRA) Forecasts (2023–2028): These provided macro-level insights into visitor volumes, nights stayed, and expenditure at the state level, based on time-series econometric modelling and historical data from 2008 to 2022. Forecasts were adjusted for exogenous uncertainties such as post-COVID aviation recovery, macroeconomic shifts, and evolving consumer preferences.\nMonthly Visitation Data: Granular monthly counts for individual caravan parks enabled improved temporal resolution and validation of seasonal variation.\nCompass IoT Trip Data (2020–2024): High-resolution geospatial vehicle movement data was used to localize and disaggregate regional forecasts down to specific caravan parks.\n\nTo extend visitation projections beyond 2028 to 2041, Exponential Smoothing State Space Models (ETS) were applied to extrapolate long-term trends, and Seasonal-Trend decomposition using LOESS (STL) was used to model and retain seasonal dynamics. This ensured continuity and robustness in both trend and seasonal forecasting components."
  },
  {
    "objectID": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#methodology-data-refinement-and-forecast-allocation",
    "href": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#methodology-data-refinement-and-forecast-allocation",
    "title": "Forecasting Caravan Park Visitors",
    "section": "Methodology: Data Refinement and Forecast Allocation",
    "text": "Methodology: Data Refinement and Forecast Allocation\nTo downscale state-level tourism forecasts to site-specific estimates, the following methodology was applied:\n\nTrip Disaggregation: Four years of Compass IoT trip data were used to compute visitation share ratios for each caravan park, based on observed trip counts during the baseline period (2020–2024). These static ratios were used to proportionally allocate aggregated TRA visitor forecasts to individual caravan park sites.\nSeasonality Adjustment: STL decomposition outputs were calibrated with monthly visitation data to apply dynamic seasonal multipliers to daily forecasts, ensuring realistic intra-annual fluctuations.\nForecast Illustration: For example, Campsite ID 452 is projected to receive an average of 28 daily visitors in April 2035, rising to 46 in December 2035, driven by end-of-year holiday peaks.\nOutput Formats: Final visitation forecasts were exported in CSV and shapefile (.shp) formats, enabling spatial visualization and integration into GIS-based planning platforms."
  },
  {
    "objectID": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#key-findings-and-insights",
    "href": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#key-findings-and-insights",
    "title": "Forecasting Caravan Park Visitors",
    "section": "Key Findings and Insights",
    "text": "Key Findings and Insights\n\nGrowth in Visitation: The integrated analysis indicates sustained growth in caravan park visitation across the HNV, in line with broader domestic tourism recovery trends.\nSite-Specific Forecasts: Disaggregated daily forecasts provide park-level detail, supporting fine-grained operational, emergency, and resource planning.\nMethodological Rigor: Data integration challenges were addressed using structured, replicable methodologies—particularly in catchment optimization, filtering logic, and seasonal calibration.\n\nTo demonstrate the practical application of our methodology, the following sections will walk through the R code used to generate these forecasts, starting with the overall NSW visitor nights.\n\nTotal NSW Visitor Nights (Domestic and International)\nThe initial step in our forecasting process involves loading historical tourism data and preparing it for time series analysis. This includes ensuring data types are correct and reshaping the data to be suitable for modeling. An Exponential Smoothing State Space Model (ETS) is then fitted to the annual visitor nights data. This model helps to project future visitor nights based on identified trends and patterns in the historical data. The resulting forecast, along with its decomposed components (like trend and level), is then visualized to assess the model’s fit and understand the projections.\n\n\nCode\nlibrary(fpp3)\nlibrary(DT)\n\n# load workspace\nload(\"tourism_forecast.RData\")\n\n# convert to integer\ntourism_fc$Nights_International &lt;- as.integer(tourism_fc$Nights_International)\ntourism_fc$Nights_Domestic &lt;- as.integer(tourism_fc$Nights_Domestic)\n\n# pivot to long format\ntourism_fc &lt;- tourism_fc |&gt;\n  pivot_longer(\n    cols = c(Nights_International, Nights_Domestic),\n    names_to = \"Type\", values_to = \"Nights\"\n  ) |&gt;\n  mutate(Type = factor(Type,\n    levels = c(\"Nights_International\", \"Nights_Domestic\")\n  ))\n\n# convert to tsibble\ntourism_fc &lt;- tourism_fc |&gt;\n  as_tsibble(index = Year, key = Type)\n\n# simple ETS model\nfit &lt;- tourism_fc |&gt;\n  # filter(Year &gt; 2021) |&gt;\n  model(ETS(Nights ~ error(\"A\") + trend(\"Ad\", phi = 0.98) + season(\"N\")))\nreport(fit)\n\n\n# A tibble: 2 × 10\n  Type              .model sigma2 log_lik   AIC  AICc   BIC    MSE   AMSE    MAE\n  &lt;fct&gt;             &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Nights_Internati… \"ETS(… 1.24e9   -249.  508.  512.  513. 9.46e8 2.52e9 15826.\n2 Nights_Domestic   \"ETS(… 1.26e8   -225.  460.  464.  465. 9.61e7 1.00e8  7311.\n\n\nCode\ncomponents(fit) |&gt; autoplot() + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nCode\nfit |&gt;\n  forecast(h = 12) |&gt;\n  autoplot(tourism_fc, level = 80) +\n  ggtitle(\"Total Visitor Nights in NSW Forecast\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nFollowing the overall forecast, the underlying data for caravan and camping tourism in NSW is examined. The script cleans and filters the tourist nights data to isolate records specific to caravan parks and commercial camping grounds in New South Wales. This refined dataset is then presented in an interactive table, allowing for easier exploration of the specific figures.\n\n\nCode\n# trim whitespace from Metric column\ntr_nights$Metric &lt;- trimws(tr_nights$Metric)\n\ntr_nights_nsw &lt;- tr_nights |&gt;\n  filter(Region == \"New South Wales\", Metric %in% \n    c(\"Caravan park or commercial camping ground\",\"Caravan or camping - non commercial\")\n  )\n\n# widen printed output\noptions(width = 250)\ndatatable(tr_nights_nsw  |&gt;\n    mutate(across(where(is.numeric), round, 0)), rownames = FALSE, options = list(scrollX = TRUE))\n\n\n\n\n\n\n\n\nMonthly Seasonal Decomposition\nTo understand intra-year dynamics, the analysis then focuses on monthly patterns. Average 12 month rolling average of the NSW Occupancy Rate shows a stable trend from May 2022 onwards.\nThe NSW Occupancy Rate data is prepared for time series analysis, and a rolling average is calculated and plotted. This helps to visualize and understand the underlying trend in occupancy rates over time, smoothing out short-term fluctuations.\n\n\nCode\n# convert to tsibble\nor_monthly_df$index &lt;- yearmonth(or_monthly_df$Month)\nor_monthly_nsw &lt;- or_monthly_df |&gt;\n  as_tsibble(index = index) |&gt;\n  select(`NSW Occupancy Rate`)\n\n# plot rolling 12 month average\nor_monthly_nsw |&gt;\n  mutate(roll_avg = slider::slide_dbl(\n    `NSW Occupancy Rate`, mean, .before = 6, .after = 5, .complete=TRUE)) |&gt;\n  mutate(roll_avg_2 = slider::slide_dbl(roll_avg, mean, .after = 1, .complete=TRUE)) |&gt;\n  autoplot(.vars=roll_avg_2) + ggtitle(\"NSW Occupancy Rate Rolling 12 Month Avg\")\n\n\n\n\n\n\n\n\n\nTo further investigate cyclical variations, the monthly occupancy rate time series is formally decomposed. This process separates the data into its constituent parts: the underlying trend, regular seasonal patterns, and the remaining irregular component. Visualizing these components helps in clearly identifying the seasonality.\n\n\nCode\nor_monthly_nsw &lt;- or_monthly_nsw |&gt;\n  filter(index &gt; yearmonth(\"2022-06\"))\n\n# seasonal decomposition\ndecomp &lt;- or_monthly_nsw |&gt;\n  model(STL(`NSW Occupancy Rate`, robust = TRUE)) |&gt;\n  components()\n\ndecomp |&gt; autoplot()\n\n\n\n\n\n\n\n\n\nNow we apply the seasonal component to the ETS forecast.\nThe seasonal pattern identified from the decomposition of monthly occupancy rates is then integrated with the annual ETS forecast. This step translates the broader annual projections into more granular monthly forecasts, reflecting typical seasonal peaks and troughs in visitor nights. The resulting seasonally adjusted monthly forecast is plotted alongside the original data for comparison.\n\n\nCode\n# Extract and normalize the seasonal component\nseasonal_component &lt;- decomp |&gt;\n  select(index, season_year) |&gt;\n  mutate(normalized_season = season_year / mean(season_year))\n\n# average seasonal component for each month\nseasonal_component &lt;- seasonal_component |&gt;\n  as_tibble() |&gt;\n  group_by(month(index)) |&gt;\n  summarise(normalized_season = mean(season_year)) |&gt;\n  ungroup()\n\n# Generate future dates\nlast_date &lt;- as.Date(max(or_monthly_nsw$index))\nfuture_dates &lt;- yearmonth(seq.Date(from = last_date + months(1), by = \"month\", length.out = 12))\n\n# Forecast using the fit model\nforecast_fit &lt;- fit |&gt;\n  forecast(h = 12)\n\n# Expand each forecasted value to 12 monthly values and apply the normalized seasonal factors\nmonthly_forecast &lt;- forecast_fit |&gt;\n  as_tibble() |&gt;\n  rowwise() |&gt;\n  mutate(monthly_forecast = list(.mean * (1+seasonal_component$normalized_season))) |&gt;\n  unnest(cols = c(monthly_forecast)) |&gt;\n  mutate(Month = rep(month(future_dates), nrow(forecast_fit))) |&gt;\n  # combine Year and Month columns\n  mutate(idx = yearmonth(paste(Year, Month, sep = \"-\"))) |&gt;\n  # distinct on idx\n  # distinct(idx, .keep_all = TRUE) |&gt;\n  as_tsibble(index = idx, key = Type)\n\n# Plot the final forecast\nmonthly_forecast |&gt;\n  autoplot(.vars=monthly_forecast) + ggtitle(\"NSW Occupancy Rate Forecast\") +\n  # add the original data\n  geom_line(\n    data = tourism_fc |&gt; mutate(yrmo = make_yearmonth(year=Year, month=1)),\n    aes(x = yrmo, y = Nights, color = Type), size = 1) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nNot enough sample in each tourism region to determine avg stay, so will use NSW data instead.\nTo convert the forecast of visitor nights into an estimate of the average number of daily visitors, data on the average length of stay is incorporated. NSW-level averages for different visitor types (domestic and international) are used. By dividing the forecasted monthly visitor nights by the average stay duration, an estimate of the average daily visitor numbers is calculated. These daily visitor forecasts are then plotted and a sample is presented in a table.\n\n\nCode\nts_stay_nsw &lt;- tr_stay |&gt; filter(\n  Region == \"New South Wales\",\n  Metric %in% c(\"Caravan park or commercial camping ground\",\"Caravan or camping - non commercial\")\n  )\n\nint_avg &lt;- (ts_stay_nsw$`International Year ending March 2024`[[1]] + \n  ts_stay_nsw$`International Year ending March 2024`[[2]]) / 2\n\ndom_avg &lt;- (ts_stay_nsw$`Domestic overnight Year ending March 2024`[[1]] + \n  ts_stay_nsw$`Domestic overnight Year ending March 2024`[[2]]) / 2\n\n# join with monthly_forecast\nmonthly_forecast$avg_stay &lt;- 0\nmonthly_forecast$avg_stay[monthly_forecast$Type == \"Nights_International\"] &lt;- int_avg\nmonthly_forecast$avg_stay[monthly_forecast$Type == \"Nights_Domestic\"] &lt;- dom_avg\n\n# calculate daily visitors\nmonthly_forecast$daily_visitors &lt;- monthly_forecast$monthly_forecast / monthly_forecast$avg_stay\n#plot\nmonthly_forecast |&gt;\n  autoplot(.vars=daily_visitors) + ggtitle(\"Daily Visitors Forecast\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nCode\ndatatable(monthly_forecast |&gt;\n    filter(Year == 2029) |&gt;\n    as_tibble() |&gt;\n    select(c(Year, Month, Type, monthly_forecast, daily_visitors)) |&gt;\n    arrange(Year, Month, Type) |&gt;\n    mutate(across(where(is.numeric), round, 0)))\n\n\n\n\n\n\nSummary:\n\nDaily Visitors to Caravan/Campervan Sites peaks in September each year.\nIn 2029 we can expect an average of 65,000 daily visitors to Caravan/Campervan Sites in NSW.\nThis is composed of ~30,000 International visitors and ~35,000 Domestic visitors.\nBy 2040, the maximum monthly average number of daily visitors is expected to increase to 86,000 (38,000 International and 48,000 Domestic visitors)."
  },
  {
    "objectID": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#per-caravan-site-forecast-methodology",
    "href": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#per-caravan-site-forecast-methodology",
    "title": "Forecasting Caravan Park Visitors",
    "section": "Per Caravan Site Forecast Methodology",
    "text": "Per Caravan Site Forecast Methodology\nTo provide more localized insights, the NSW-level daily visitor forecast can be disaggregated to individual caravan sites. While the specific forecast outputs are not presented here due to data sensitivity, the methodology involves using historical trip ratios derived from sources like CompassIoT data.\nThe process begins by calculating site-specific trip ratios from historical data. This involves determining the proportion of total observed trips (e.g. vehicle trips to caravan park areas) that can be attributed to each individual caravan site over a defined baseline period.\nThese proportional ratios are then applied to the overall NSW daily visitor forecast (derived in the previous steps of this analysis). By multiplying the total NSW daily visitor numbers for a given period by each site’s proportional trip ratio, an estimate of daily visitors for that specific site is obtained.\nThis disaggregation allows for a more localized understanding of tourism patterns. The resulting site-specific forecasts can then be prepared for various uses, including tabular analysis for operational planning and geospatial visualization by joining them with caravan site location data, which aids in regional resource management and infrastructure planning. This methodological approach enables the translation of broader regional forecasts into actionable, site-level insights."
  },
  {
    "objectID": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#data-integration-process-and-challenges",
    "href": "posts/forecasting-caravan-visitors/forecasting-caravan-visitors.html#data-integration-process-and-challenges",
    "title": "Forecasting Caravan Park Visitors",
    "section": "Data Integration Process and Challenges",
    "text": "Data Integration Process and Challenges\nThe integration of Compass IoT data, monthly visitation records, and TRA forecasts required addressing several technical challenges:\n\nCompass IoT Data Characteristics: The dataset comprises telemetry information (location, speed, timestamp, acceleration) from over 64 vehicle manufacturers.\nExport Constraints: Compass IoT’s CSV export is capped at 1,000 links per query, requiring a series of batched exports to obtain full trip datasets for all caravan park catchments.\nCatchment Delineation: Iterative tuning of park-level catchment radii was performed to optimize inclusion of relevant trips while avoiding overlapping or misattributed traffic.\nThrough-Traffic Filtering: Only trips terminating or originating within designated catchment zones were retained, excluding pass-through movements not associated with park visitation.\nMulti-Year Ratio Stability: Aggregated visitation ratios were validated across multiple years to ensure consistency and reliability before applying to forecast disaggregation.\nCross-Validation of Seasonality: Seasonal trends identified from STL decomposition were reconciled with monthly observed data to confirm alignment in peak periods and off-seasons."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Blog about Artificial Intelligence",
    "section": "",
    "text": "Forecasting Caravan Park Visitors\n\n\n\n\n\n\ncode\n\n\nforecasting\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nMay 18, 2025\n\n\nAgentRichi, AgentZizi\n\n\n\n\n\n\n\n\n\n\n\n\nHow to win a Kaggle NLP Competition\n\n\n\n\n\n\ncode\n\n\nfastai\n\n\nNLP\n\n\nkaggle\n\n\n\n\n\n\n\n\n\nSep 1, 2024\n\n\nAgentRichi\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to MATSim\n\n\n\n\n\n\ncode\n\n\nsoftware\n\n\n\n\n\n\n\n\n\nAug 23, 2024\n\n\nAgentZizi\n\n\n\n\n\n\n\n\n\n\n\n\nWho Directed That?\n\n\n\n\n\n\ncode\n\n\ndeep learning\n\n\nfastai\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nAgentRichi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the Synthetic AI Agents Blog about machine learning and agent based modelling. Watch your head, the ceiling for joke quality is quite low here!"
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html",
    "title": "How to win a Kaggle NLP Competition",
    "section": "",
    "text": "Back in 2020 I took part in an online data science competition where the aim was to correctly identify Tweets announcing real disasters from all the other noise on Twitter. This article outlines how I came up with a submission that landed me a spot in the top 5."
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#correct-mislabelled-training-data",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#correct-mislabelled-training-data",
    "title": "How to win a Kaggle NLP Competition",
    "section": "1. Correct mislabelled training data",
    "text": "1. Correct mislabelled training data\nThis step is pretty staightforward, but can be very time consuming if done manually (especially if the training dataset is big). Luckily Kaggle has an awesome community, and this is a perfect example of why it pays off to be active on the competition forums. If there’s issues with the training data, someone will nearly always discover this eventually and make a post about it. This was the case here as well. In the end the data does not need to be 100% accurately labelled, but depending on if there is any bias in the mislabelling, even chaning a small amount of incorrect labels can make a big difference. The tradeoff, of course, is time."
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#gather-label-and-append-additional-training-data",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#gather-label-and-append-additional-training-data",
    "title": "How to win a Kaggle NLP Competition",
    "section": "2. Gather, label and append additional training data",
    "text": "2. Gather, label and append additional training data\nI actually came back to this later, once I had a working model and a baseline accuracy score, and it made a tremendous difference to my final results.\nThe dataset consisted of around 10,000 tweets gathered by keyword search on words such as “quarantine”, “ablaze”, etc. To gather additional data, I simply repeated this exercise using Twitter’s advanced search to collect and label about 300 more tweets. The incidence rate (true positives) of actual disasters as a proportion of returned tweets is quite low. This is likely the main reason why adding more training data (even if it’s only ~5%) made a big difference to model accuracy.\nAdditionally, as sumbissions were limited to using Google’s automated neural net search to find the best model for the given dataset, adding more training data was one of the best ways to get ahead of the competition."
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#use-fastai-to-build-a-text-classifier-model",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#use-fastai-to-build-a-text-classifier-model",
    "title": "How to win a Kaggle NLP Competition",
    "section": "3. Use fastai to build a text classifier model",
    "text": "3. Use fastai to build a text classifier model\nFastai, as the name suggests, is a fantastic library for quickly getting a working ML model for many tasks when working with text or image data. Not only is it fast, but often the default parameters actually yield close to state-of-the art results.\nEven though I wouldn’t be able to use fastai or any other library (pytorch, keras, etc.) to build a model for my final submission, AutoML is actually quite terrible for iterative model development. For example, if I wanted to add a new text cleaning function to see if it improves my model using AutoML, I would have to submit the new dataset to Google’s server and wait for a new model to be trained. This can take several hours, terrible for any ML workflow!\nWith fastai, I was able to quickly get a text classification model set up using the text learner api. Now instead of waiting hours to see the results of any minor dataset tweaks, I can get an updated baseline in seconds. Each training cycle using fastai on this dataset took about 10 seconds, and even after 3-4 cycles the model was usually not improving anymore. Here’s how simple it is to set up:\nfrom fastai.text import *\nimport pandas as pd\n\ndf_train = pd.read_csv('train.csv')\n\ndata_clas = (TextList.from_df(df_train, cols='text', vocab=data_lm.vocab)\n           #Inputs: the train.csv file\n            .split_by_rand_pct(0.1)\n           #We randomly split and keep 10% (10,000 reviews) for validation\n            .label_from_df(cols='target')          \n           #We want to do a language model so we label accordingly\n            .databunch(bs=bs))\n\nlearn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\nlearn.load_encoder('fine_tuned_enc')\n\nlearn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))\nAll the code is hosted in this kaggle notebook. After a few more cycles of freezing and fitting, the model achieved an accuracy score of 0.816 (i.e. 81.6% correct labels), which isn’t far off from our final model’s score (which incorporates all the additional training data and text cleaning) of 0.875!"
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#transform-tweets-using-spelling-and-regex-functions",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#transform-tweets-using-spelling-and-regex-functions",
    "title": "How to win a Kaggle NLP Competition",
    "section": "4. Transform tweets using spelling and regex functions",
    "text": "4. Transform tweets using spelling and regex functions\nBesides gathering more tweets, data transformation was one of the only ways to improve performance. The field of NLP has been getting a lot of attention recently, and a lot of great resources are available on the topic. Great topics to explore further include text stemming, lemmatization, spelling and regex. In most scenarios the current best practicies will generally yield the best results. For my final submission, I used the methods defined in the NLTK 3 Cookbook, a great resource for text transformation and cleaning. Here’s how to set this up for any generic text dataset:\nimport nltk\nfrom nltk import word_tokenize\nimport enchant\nfrom tqdm import tqdm\nfrom nltk.metrics import edit_distance\n\nreplacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g&lt;1&gt; will'),\n    (r'(\\w+)n\\'t', '\\g&lt;1&gt; not'),\n    (r'(\\w+)\\'ve', '\\g&lt;1&gt; have'),\n    (r'(\\w+)\\'s', '\\g&lt;1&gt; is'),\n    (r'(\\w+)\\'re', '\\g&lt;1&gt; are'),\n    (r'(\\w+)\\'d', '\\g&lt;1&gt; would'),\n]\n\nclass RegexpReplacer(object):\n    # Replaces regular expression in a text.\n    def __init__(self, patterns=replacement_patterns):\n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    \n    def replace(self, text):\n        s = text\n        \n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s)\n        \n        return s\n\nclass SpellingReplacer(object):\n    \"\"\" Replaces misspelled words with a likely suggestion based on shortest\n    edit distance\n    \"\"\"\n    def __init__(self, dict_name='en', max_dist=2):\n        self.spell_dict = enchant.Dict(dict_name)\n        self.max_dist = max_dist\n    \n    def replace(self, word):\n        if self.spell_dict.check(word):\n            return word\n        \n        suggestions = self.spell_dict.suggest(word)\n        \n   \n\n     if suggestions and edit_distance(word, suggestions[0]) &lt;= self.max_dist:\n            return suggestions[0]\n        else:\n            return word\n\ndef clean_tweet(text) :\n    # remove urls\n    #text = df.apply(lambda x: re.sub(r'http\\S+', '', x))\n    text = re.sub(r'http\\S+', '', text)\n\n    # replace contractions\n    replacer = RegexpReplacer()\n    text = replacer.replace(text)\n\n    #split words on - and \\\n    text = re.sub(r'\\b', ' ', text)\n    text = re.sub(r'-', ' ', text)\n\n    # replace negations with antonyms\n\n    #nltk.download('punkt')\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    tokens = tokenizer.tokenize(text)\n\n    # spelling correction\n    replacer = SpellingReplacer()\n    tokens = [replacer.replace(t) for t in tokens]\n\n    # lemmatize/stemming\n    wnl = nltk.WordNetLemmatizer()\n    tokens = [wnl.lemmatize(t) for t in tokens]\n    porter = nltk.PorterStemmer()\n    tokens = [porter.stem(t) for t in tokens]\n    # filter insignificant words (using fastai)\n    # swap word phrases\n\n    text = ' '.join(tokens)\n    return(text)\nThen to apply this to the entire dataset is super straightforward:\ntweets = df_train['text']\ntqdm.pandas(desc=\"Cleaning tweets\")\ntweets_cleaned = tweets.progress_apply(clean_tweet)\ndf_train['text_clean'] = tweets_cleaned\nCleaning tweets: 100%|██████████| 10860/10860 [16:53&lt;00:00, 10.71it/s]\nHere’s a couple of examples of what how this changes our input data: &gt;Just happened a terrible car crash &gt; just happen a terribl car crash\n\nHeard about #earthquake is different cities, stay safe &gt; heard about earthquak is differ citi stay safe\n\nLanguague models don’t really understand the subtle differences in word endings, tenses and so on. What text cleaning really does is standardize words to make them easier for a machine to interpret. So for example the words “happened”, “happens”, “happen” are all transformed to “happen”.\nThe final step is to convert our cleaned text data into tokens. There’s lots of literature on tokenizing text data that explains this better than I can, but basically this is transforming each work into a unique numeric token that can be read and understood by our neural net. Hence the importance of our text cleaning earlier, else the same word would be tokenized multiple times for each ending/spelling (happened, happens, happen). For this I used the spacy library:\nimport spacy\ntrain = []\ntokenizer = Tokenizer()\ntok = SpacyTokenizer('en')\nfor line in tqdm(df_train.text_clean):\n    lne = ' '.join(tokenizer.process_text(line, tok))\n    train.append(lne)\n\ndf_train['text_tokens'] = train\n100%|██████████| 10860/10860 [00:02&lt;00:00, 4245.57it/s]"
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#set-up-and-submit-trainig-data-to-automl",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#set-up-and-submit-trainig-data-to-automl",
    "title": "How to win a Kaggle NLP Competition",
    "section": "5. Set up and submit trainig data to AutoML",
    "text": "5. Set up and submit trainig data to AutoML\nFinally we’re ready to train our model! This was my first time using Goolge Cloud and AutoML, but luckily as part of the competition a super handy AutoML Getting Started Notebook was provided. I copied this workflow and followed the additional instructions to train my model on AutoML. Unlike on most ML projects, there really isn’t much to discuss about architecture here, as AutoML is mainly a black box. I’ll just list the main steps in the AutoML workflow here:\n\nUpload data to a Google Cloud Blob\nCreate a model instance using AutoMLWrapper\nImport our uploaded dataset\nTrain the AutoML model\nUse the model api for making predictions\n\nAnd that’s it!"
  },
  {
    "objectID": "posts/MATSim/MATSim.html",
    "href": "posts/MATSim/MATSim.html",
    "title": "Intro to MATSim",
    "section": "",
    "text": "What do you know about MATSim?\n\nIntro\nhenlo frienii"
  }
]