[
  {
    "objectID": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html",
    "href": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html",
    "title": "Who Directed That?",
    "section": "",
    "text": "Starting off the Synagen blog with a fun one. This is really just a small toy example working through the FastAI practical deep learning course, specifically Lesson 1 and Lesson 2 content.\nIn this post I show (interactively) how easy it is to train a simple object detection neural net in just a few lines of python code.\nDeep Learning has made some serious advances in recent years when it comes to image and video data, so it shouldn’t surprise anyone how well architectures like Convolutional Neural Networks (CNNs) can easily identify humans/animals/objects in photos.\nSo instead of doing yet another dog/cat classifier, I set out to challenge the AI a bit: could it detect who directed a movie based only on a screenshot, and only after being trained on other movies from the same director?\nThat last bit is quite important - I assume it would be no challenge to detect scenes from the same movie that the model was trained on, but will the learnings about a director’s artistic style translate even to new movies that the model has never seen before?\nTo find out, I picked two of my favourite directors - Wes Anderson and David Lynch - , then downloaded a bunch of images of scenes from some of their most iconic works, and trained a deep learning model to classify each scene.\nRead on to find out how it went…\n\n\n\nfrom fastbook import *\nfrom fastai.vision import *\nfrom fastai.vision.widgets import *\nfrom decouple import config, AutoConfig\n\n\n\n\nHere we use Bing to search for images. I found that adding the director’s name and ‘scene’ at the end of the search query yielded best results. Below is a sample image from Anderson’s earliest film: Bottle Rocket\n\nconfig = AutoConfig(' ')\nkey = config('AZURE_SEARCH_KEY')\n\n\nresults = search_images_bing(key, 'wes anderson bottle rocket scene')\nims = results.attrgot('contentUrl')\nlen(ims)\n\n150\n\n\n\ndest = 'bottlerocket.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      100.09% [2531328/2529007 00:02&lt;00:00]\n    \n    \n\n\nPath('bottlerocket.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(512,512)\n\n\n\n\n\n\n\n\nLet’s get the rest of the movie scenes downloaded!\nHere I separate our data into training and validation sets. Specifically, I picked The French Dispatch and Moonrise Kingdom as Wes Anderson validation movies, and similarly singled out Mulholland Drive and Blue Velvet from Lynch’s colletion.\nI did find that if we downloaded too many images from each movie, they would start getting repetitive/not relevant (e.g. pictures of movie posters rather than actual scenes from the movies), so I limit the search to the first 30 results from each.\n\nwa_movies_train = ['bottle rocket','rushmore','the royal tenebaums', 'the life aquatic with steve zissou',\n            'the darjeeling limited','the grand budapest hotel','asteroid city']\nwa_movies_val = ['the french dispatch','moonrise kingdom',]\n\ndl_movies_train = ['eraserhead','the elephant man','wild at heart',\n            'twin peaks','lost highway','inland empire']\ndl_movies_val = ['mulholland drive','blue velvet',]\n\npath = Path('movies')\n\n\nif not path.exists():\n    path.mkdir()\ntrain_path = (path/'training')\nif not train_path.exists():\n    train_path.mkdir()\n    for o in wa_movies_train:\n        dest = (train_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_train:\n        dest = (train_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\n\nval_path = (path/'validation')\nif not val_path.exists():\n    val_path.mkdir()\n    for o in wa_movies_val:\n        dest = (val_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_val:\n        dest = (val_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\nHere we remove any broken links:\n\nfns = get_image_files(path)\nfailed = verify_images(fns)\nfailed.map(Path.unlink);\n\n\n\n\nNow that we have all our data collected, let’s prepare it for our ML model using fastai’s DataBlock class:\nTo break it down, we specify the following: - The type of blocks (input = images, output = categories) - How to get our data? Using the get_image_files function - How to split training/validation? Based on the folder names 2 levels up - How to label our data? Based on the folder names 1 level up - How to transform our data? Resize all images to a standard 256x256 pixels\n\nscenes = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=GrandparentSplitter(train_name='training', valid_name='validation'),\n    get_y=parent_label,\n    item_tfms=Resize(256))\n\nNow that we have everything specified, let’s load in our data (making sure to suffle both train and val sets) and display a batch of images:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\n\n\ndl_train.train.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\nLooking good…\nThere may be some images in here that we want to remove (e.g. ones with movie titles, or multiple scenes in one image), but we will deal with that later.\nFirst, let’s actually train our model and see how it does!\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(6)\n\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.082164\n0.818760\n0.500000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.526650\n0.399322\n0.500000\n00:10\n\n\n1\n0.397128\n0.348378\n0.500000\n00:10\n\n\n2\n0.327281\n0.306815\n0.500000\n00:10\n\n\n3\n0.260668\n0.295519\n0.500000\n00:10\n\n\n4\n0.215509\n0.300533\n0.500000\n00:10\n\n\n5\n0.190804\n0.298368\n0.500000\n00:10\n\n\n\n\n\nSomething interesting here is that even though we trained for 6 epochs, the valiation loss actually stopped improving after only 3. We could have stopped training there - the training loss still improves but this is likely just overfitting on our input data.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo that took about a minute to train*, and we now have a machine learning model that can tell the difference between Wes Anderson and David Lynch in over 95% of our examples.\n*we did use a pretrained model so that saved a LOT of time!\nLet’s now go through our dataset and remove any shots of filming, duplicates, or other images that are just adding noise.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nNow let’s run our data prep -&gt; model training -&gt; accuracy scoring steps again:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\ndl_train.valid.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.234754\n1.198495\n0.490000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.517711\n0.332291\n0.490000\n00:10\n\n\n1\n0.383631\n0.248246\n0.490000\n00:10\n\n\n2\n0.298077\n0.262733\n0.490000\n00:10\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd what do the results actually look like? Below we take a look at some validation set images, with the actual and predicted labels printed above the pictures. Not bad!\n\ninterp.show_results(list(range(0,100,4)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at which images the model struggles with by using plot_top_losses. I’d say that even diehard fans might struggle with a couple of these!\n\ninterp.plot_top_losses(6, nrows=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first shot of Saoirse Ronan from The French Dispatch looks a lot like Naomi Watts in Mulholland Drive, while the middle lower image of Timothée Chalamet could have been taken straight out of Lynch’s Eraserhead!\n\n#hide\nf, axarr = plt.subplots(2,2)\nf.set_figheight(8)\nf.set_figwidth(12)\naxarr[0,0].imshow(Image.open(Path('movies')/'validation/wesanderson/20fa22db-b9d3-4eae-9537-d4c2c68095c4.jpg'))\naxarr[0,0].set_title(\"The French Dispatch\")\naxarr[0,0].axis('off')\naxarr[0,1].imshow(Image.open(Path('movies')/'validation/davidlynch/b8f11185-392e-4be4-a2d6-99b00cb04783.jpg'))\naxarr[0,1].set_title(\"Mulholland Drive\")\naxarr[0,1].axis('off')\naxarr[1,0].imshow(Image.open(Path('movies')/'validation/wesanderson/3e787a80-3f2e-4dc5-a27d-82ea56733a15.jpg'))\naxarr[1,0].set_title(\"The French Dispatch\")\naxarr[1,0].axis('off')\naxarr[1,1].imshow(Image.open(Path('movies')/'training/davidlynch/2e17a81a-c911-4f23-afe5-5c89bdc07bdf.png'))\naxarr[1,1].set_title(\"Eraserhead\")\naxarr[1,1].axis('off')\n\n\n\n\n\n\n\n\nCoincidence? Or perhaps some artistic influence from Lynch’s earlier works infiltrated Anderson’s mind when producing The French Dispatch? Well if even our state-of-the-art ML model cannot tell them apart, perhaps there is some evidence of inspiration, but I guess we’ll never know for sure…"
  },
  {
    "objectID": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html#a-ml-model-to-detect-whether-thats-a-wes-anderson-or-a-david-lynch-movie",
    "href": "posts/wes-anderson-or-daivd-lynch/WesAnderson_or_DavidLynch.html#a-ml-model-to-detect-whether-thats-a-wes-anderson-or-a-david-lynch-movie",
    "title": "Who Directed That?",
    "section": "",
    "text": "Starting off the Synagen blog with a fun one. This is really just a small toy example working through the FastAI practical deep learning course, specifically Lesson 1 and Lesson 2 content.\nIn this post I show (interactively) how easy it is to train a simple object detection neural net in just a few lines of python code.\nDeep Learning has made some serious advances in recent years when it comes to image and video data, so it shouldn’t surprise anyone how well architectures like Convolutional Neural Networks (CNNs) can easily identify humans/animals/objects in photos.\nSo instead of doing yet another dog/cat classifier, I set out to challenge the AI a bit: could it detect who directed a movie based only on a screenshot, and only after being trained on other movies from the same director?\nThat last bit is quite important - I assume it would be no challenge to detect scenes from the same movie that the model was trained on, but will the learnings about a director’s artistic style translate even to new movies that the model has never seen before?\nTo find out, I picked two of my favourite directors - Wes Anderson and David Lynch - , then downloaded a bunch of images of scenes from some of their most iconic works, and trained a deep learning model to classify each scene.\nRead on to find out how it went…\n\n\n\nfrom fastbook import *\nfrom fastai.vision import *\nfrom fastai.vision.widgets import *\nfrom decouple import config, AutoConfig\n\n\n\n\nHere we use Bing to search for images. I found that adding the director’s name and ‘scene’ at the end of the search query yielded best results. Below is a sample image from Anderson’s earliest film: Bottle Rocket\n\nconfig = AutoConfig(' ')\nkey = config('AZURE_SEARCH_KEY')\n\n\nresults = search_images_bing(key, 'wes anderson bottle rocket scene')\nims = results.attrgot('contentUrl')\nlen(ims)\n\n150\n\n\n\ndest = 'bottlerocket.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      100.09% [2531328/2529007 00:02&lt;00:00]\n    \n    \n\n\nPath('bottlerocket.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(512,512)\n\n\n\n\n\n\n\n\nLet’s get the rest of the movie scenes downloaded!\nHere I separate our data into training and validation sets. Specifically, I picked The French Dispatch and Moonrise Kingdom as Wes Anderson validation movies, and similarly singled out Mulholland Drive and Blue Velvet from Lynch’s colletion.\nI did find that if we downloaded too many images from each movie, they would start getting repetitive/not relevant (e.g. pictures of movie posters rather than actual scenes from the movies), so I limit the search to the first 30 results from each.\n\nwa_movies_train = ['bottle rocket','rushmore','the royal tenebaums', 'the life aquatic with steve zissou',\n            'the darjeeling limited','the grand budapest hotel','asteroid city']\nwa_movies_val = ['the french dispatch','moonrise kingdom',]\n\ndl_movies_train = ['eraserhead','the elephant man','wild at heart',\n            'twin peaks','lost highway','inland empire']\ndl_movies_val = ['mulholland drive','blue velvet',]\n\npath = Path('movies')\n\n\nif not path.exists():\n    path.mkdir()\ntrain_path = (path/'training')\nif not train_path.exists():\n    train_path.mkdir()\n    for o in wa_movies_train:\n        dest = (train_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_train:\n        dest = (train_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\n\nval_path = (path/'validation')\nif not val_path.exists():\n    val_path.mkdir()\n    for o in wa_movies_val:\n        dest = (val_path/'wesanderson')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'wes anderson {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n    for o in dl_movies_val:\n        dest = (val_path/'davidlynch')\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'david lynch {o} scene', max_images=30)\n        download_images(dest, urls=results.attrgot('contentUrl'))\n\nHere we remove any broken links:\n\nfns = get_image_files(path)\nfailed = verify_images(fns)\nfailed.map(Path.unlink);\n\n\n\n\nNow that we have all our data collected, let’s prepare it for our ML model using fastai’s DataBlock class:\nTo break it down, we specify the following: - The type of blocks (input = images, output = categories) - How to get our data? Using the get_image_files function - How to split training/validation? Based on the folder names 2 levels up - How to label our data? Based on the folder names 1 level up - How to transform our data? Resize all images to a standard 256x256 pixels\n\nscenes = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=GrandparentSplitter(train_name='training', valid_name='validation'),\n    get_y=parent_label,\n    item_tfms=Resize(256))\n\nNow that we have everything specified, let’s load in our data (making sure to suffle both train and val sets) and display a batch of images:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\n\n\ndl_train.train.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\nLooking good…\nThere may be some images in here that we want to remove (e.g. ones with movie titles, or multiple scenes in one image), but we will deal with that later.\nFirst, let’s actually train our model and see how it does!\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(6)\n\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/richardrossmann/miniforge_x86_64/envs/pytorch_x86/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.082164\n0.818760\n0.500000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.526650\n0.399322\n0.500000\n00:10\n\n\n1\n0.397128\n0.348378\n0.500000\n00:10\n\n\n2\n0.327281\n0.306815\n0.500000\n00:10\n\n\n3\n0.260668\n0.295519\n0.500000\n00:10\n\n\n4\n0.215509\n0.300533\n0.500000\n00:10\n\n\n5\n0.190804\n0.298368\n0.500000\n00:10\n\n\n\n\n\nSomething interesting here is that even though we trained for 6 epochs, the valiation loss actually stopped improving after only 3. We could have stopped training there - the training loss still improves but this is likely just overfitting on our input data.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo that took about a minute to train*, and we now have a machine learning model that can tell the difference between Wes Anderson and David Lynch in over 95% of our examples.\n*we did use a pretrained model so that saved a LOT of time!\nLet’s now go through our dataset and remove any shots of filming, duplicates, or other images that are just adding noise.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nNow let’s run our data prep -&gt; model training -&gt; accuracy scoring steps again:\n\ndl_train = scenes.dataloaders(path, shuffle=True, val_shuffle=True)\ndl_train.valid.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dl_train, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.234754\n1.198495\n0.490000\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.517711\n0.332291\n0.490000\n00:10\n\n\n1\n0.383631\n0.248246\n0.490000\n00:10\n\n\n2\n0.298077\n0.262733\n0.490000\n00:10\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd what do the results actually look like? Below we take a look at some validation set images, with the actual and predicted labels printed above the pictures. Not bad!\n\ninterp.show_results(list(range(0,100,4)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at which images the model struggles with by using plot_top_losses. I’d say that even diehard fans might struggle with a couple of these!\n\ninterp.plot_top_losses(6, nrows=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first shot of Saoirse Ronan from The French Dispatch looks a lot like Naomi Watts in Mulholland Drive, while the middle lower image of Timothée Chalamet could have been taken straight out of Lynch’s Eraserhead!\n\n#hide\nf, axarr = plt.subplots(2,2)\nf.set_figheight(8)\nf.set_figwidth(12)\naxarr[0,0].imshow(Image.open(Path('movies')/'validation/wesanderson/20fa22db-b9d3-4eae-9537-d4c2c68095c4.jpg'))\naxarr[0,0].set_title(\"The French Dispatch\")\naxarr[0,0].axis('off')\naxarr[0,1].imshow(Image.open(Path('movies')/'validation/davidlynch/b8f11185-392e-4be4-a2d6-99b00cb04783.jpg'))\naxarr[0,1].set_title(\"Mulholland Drive\")\naxarr[0,1].axis('off')\naxarr[1,0].imshow(Image.open(Path('movies')/'validation/wesanderson/3e787a80-3f2e-4dc5-a27d-82ea56733a15.jpg'))\naxarr[1,0].set_title(\"The French Dispatch\")\naxarr[1,0].axis('off')\naxarr[1,1].imshow(Image.open(Path('movies')/'training/davidlynch/2e17a81a-c911-4f23-afe5-5c89bdc07bdf.png'))\naxarr[1,1].set_title(\"Eraserhead\")\naxarr[1,1].axis('off')\n\n\n\n\n\n\n\n\nCoincidence? Or perhaps some artistic influence from Lynch’s earlier works infiltrated Anderson’s mind when producing The French Dispatch? Well if even our state-of-the-art ML model cannot tell them apart, perhaps there is some evidence of inspiration, but I guess we’ll never know for sure…"
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html",
    "title": "How to win a Kaggle NLP Competition",
    "section": "",
    "text": "Back in 2020 I took part in an online data science competition where the aim was to correctly identify Tweets announcing real disasters from all the other noise on Twitter. This article outlines how I came up with a submission that landed me a spot in the top 5."
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#correct-mislabelled-training-data",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#correct-mislabelled-training-data",
    "title": "How to win a Kaggle NLP Competition",
    "section": "1. Correct mislabelled training data",
    "text": "1. Correct mislabelled training data\nThis step is pretty staightforward, but can be very time consuming if done manually (especially if the training dataset is big). Luckily Kaggle has an awesome community, and this is a perfect example of why it pays off to be active on the competition forums. If there’s issues with the training data, someone will nearly always discover this eventually and make a post about it. This was the case here as well. In the end the data does not need to be 100% accurately labelled, but depending on if there is any bias in the mislabelling, even chaning a small amount of incorrect labels can make a big difference. The tradeoff, of course, is time."
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#gather-label-and-append-additional-training-data",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#gather-label-and-append-additional-training-data",
    "title": "How to win a Kaggle NLP Competition",
    "section": "2. Gather, label and append additional training data",
    "text": "2. Gather, label and append additional training data\nI actually came back to this later, once I had a working model and a baseline accuracy score, and it made a tremendous difference to my final results.\nThe dataset consisted of around 10,000 tweets gathered by keyword search on words such as “quarantine”, “ablaze”, etc. To gather additional data, I simply repeated this exercise using Twitter’s advanced search to collect and label about 300 more tweets. The incidence rate (true positives) of actual disasters as a proportion of returned tweets is quite low. This is likely the main reason why adding more training data (even if it’s only ~5%) made a big difference to model accuracy.\nAdditionally, as sumbissions were limited to using Google’s automated neural net search to find the best model for the given dataset, adding more training data was one of the best ways to get ahead of the competition."
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#use-fastai-to-build-a-text-classifier-model",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#use-fastai-to-build-a-text-classifier-model",
    "title": "How to win a Kaggle NLP Competition",
    "section": "3. Use fastai to build a text classifier model",
    "text": "3. Use fastai to build a text classifier model\nFastai, as the name suggests, is a fantastic library for quickly getting a working ML model for many tasks when working with text or image data. Not only is it fast, but often the default parameters actually yield close to state-of-the art results.\nEven though I wouldn’t be able to use fastai or any other library (pytorch, keras, etc.) to build a model for my final submission, AutoML is actually quite terrible for iterative model development. For example, if I wanted to add a new text cleaning function to see if it improves my model using AutoML, I would have to submit the new dataset to Google’s server and wait for a new model to be trained. This can take several hours, terrible for any ML workflow!\nWith fastai, I was able to quickly get a text classification model set up using the text learner api. Now instead of waiting hours to see the results of any minor dataset tweaks, I can get an updated baseline in seconds. Each training cycle using fastai on this dataset took about 10 seconds, and even after 3-4 cycles the model was usually not improving anymore. Here’s how simple it is to set up:\nfrom fastai.text import *\nimport pandas as pd\n\ndf_train = pd.read_csv('train.csv')\n\ndata_clas = (TextList.from_df(df_train, cols='text', vocab=data_lm.vocab)\n           #Inputs: the train.csv file\n            .split_by_rand_pct(0.1)\n           #We randomly split and keep 10% (10,000 reviews) for validation\n            .label_from_df(cols='target')          \n           #We want to do a language model so we label accordingly\n            .databunch(bs=bs))\n\nlearn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\nlearn.load_encoder('fine_tuned_enc')\n\nlearn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))\nAll the code is hosted in this kaggle notebook. After a few more cycles of freezing and fitting, the model achieved an accuracy score of 0.816 (i.e. 81.6% correct labels), which isn’t far off from our final model’s score (which incorporates all the additional training data and text cleaning) of 0.875!"
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#transform-tweets-using-spelling-and-regex-functions",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#transform-tweets-using-spelling-and-regex-functions",
    "title": "How to win a Kaggle NLP Competition",
    "section": "4. Transform tweets using spelling and regex functions",
    "text": "4. Transform tweets using spelling and regex functions\nBesides gathering more tweets, data transformation was one of the only ways to improve performance. The field of NLP has been getting a lot of attention recently, and a lot of great resources are available on the topic. Great topics to explore further include text stemming, lemmatization, spelling and regex. In most scenarios the current best practicies will generally yield the best results. For my final submission, I used the methods defined in the NLTK 3 Cookbook, a great resource for text transformation and cleaning. Here’s how to set this up for any generic text dataset:\nimport nltk\nfrom nltk import word_tokenize\nimport enchant\nfrom tqdm import tqdm\nfrom nltk.metrics import edit_distance\n\nreplacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g&lt;1&gt; will'),\n    (r'(\\w+)n\\'t', '\\g&lt;1&gt; not'),\n    (r'(\\w+)\\'ve', '\\g&lt;1&gt; have'),\n    (r'(\\w+)\\'s', '\\g&lt;1&gt; is'),\n    (r'(\\w+)\\'re', '\\g&lt;1&gt; are'),\n    (r'(\\w+)\\'d', '\\g&lt;1&gt; would'),\n]\n\nclass RegexpReplacer(object):\n    # Replaces regular expression in a text.\n    def __init__(self, patterns=replacement_patterns):\n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    \n    def replace(self, text):\n        s = text\n        \n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s)\n        \n        return s\n\nclass SpellingReplacer(object):\n    \"\"\" Replaces misspelled words with a likely suggestion based on shortest\n    edit distance\n    \"\"\"\n    def __init__(self, dict_name='en', max_dist=2):\n        self.spell_dict = enchant.Dict(dict_name)\n        self.max_dist = max_dist\n    \n    def replace(self, word):\n        if self.spell_dict.check(word):\n            return word\n        \n        suggestions = self.spell_dict.suggest(word)\n        \n   \n\n     if suggestions and edit_distance(word, suggestions[0]) &lt;= self.max_dist:\n            return suggestions[0]\n        else:\n            return word\n\ndef clean_tweet(text) :\n    # remove urls\n    #text = df.apply(lambda x: re.sub(r'http\\S+', '', x))\n    text = re.sub(r'http\\S+', '', text)\n\n    # replace contractions\n    replacer = RegexpReplacer()\n    text = replacer.replace(text)\n\n    #split words on - and \\\n    text = re.sub(r'\\b', ' ', text)\n    text = re.sub(r'-', ' ', text)\n\n    # replace negations with antonyms\n\n    #nltk.download('punkt')\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    tokens = tokenizer.tokenize(text)\n\n    # spelling correction\n    replacer = SpellingReplacer()\n    tokens = [replacer.replace(t) for t in tokens]\n\n    # lemmatize/stemming\n    wnl = nltk.WordNetLemmatizer()\n    tokens = [wnl.lemmatize(t) for t in tokens]\n    porter = nltk.PorterStemmer()\n    tokens = [porter.stem(t) for t in tokens]\n    # filter insignificant words (using fastai)\n    # swap word phrases\n\n    text = ' '.join(tokens)\n    return(text)\nThen to apply this to the entire dataset is super straightforward:\ntweets = df_train['text']\ntqdm.pandas(desc=\"Cleaning tweets\")\ntweets_cleaned = tweets.progress_apply(clean_tweet)\ndf_train['text_clean'] = tweets_cleaned\nCleaning tweets: 100%|██████████| 10860/10860 [16:53&lt;00:00, 10.71it/s]\nHere’s a couple of examples of what how this changes our input data: &gt;Just happened a terrible car crash &gt; just happen a terribl car crash\n\nHeard about #earthquake is different cities, stay safe &gt; heard about earthquak is differ citi stay safe\n\nLanguague models don’t really understand the subtle differences in word endings, tenses and so on. What text cleaning really does is standardize words to make them easier for a machine to interpret. So for example the words “happened”, “happens”, “happen” are all transformed to “happen”.\nThe final step is to convert our cleaned text data into tokens. There’s lots of literature on tokenizing text data that explains this better than I can, but basically this is transforming each work into a unique numeric token that can be read and understood by our neural net. Hence the importance of our text cleaning earlier, else the same word would be tokenized multiple times for each ending/spelling (happened, happens, happen). For this I used the spacy library:\nimport spacy\ntrain = []\ntokenizer = Tokenizer()\ntok = SpacyTokenizer('en')\nfor line in tqdm(df_train.text_clean):\n    lne = ' '.join(tokenizer.process_text(line, tok))\n    train.append(lne)\n\ndf_train['text_tokens'] = train\n100%|██████████| 10860/10860 [00:02&lt;00:00, 4245.57it/s]"
  },
  {
    "objectID": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#set-up-and-submit-trainig-data-to-automl",
    "href": "posts/first-kaggle-competition/how-to-win-a-kaggle-NLP-competition.html#set-up-and-submit-trainig-data-to-automl",
    "title": "How to win a Kaggle NLP Competition",
    "section": "5. Set up and submit trainig data to AutoML",
    "text": "5. Set up and submit trainig data to AutoML\nFinally we’re ready to train our model! This was my first time using Goolge Cloud and AutoML, but luckily as part of the competition a super handy AutoML Getting Started Notebook was provided. I copied this workflow and followed the additional instructions to train my model on AutoML. Unlike on most ML projects, there really isn’t much to discuss about architecture here, as AutoML is mainly a black box. I’ll just list the main steps in the AutoML workflow here:\n\nUpload data to a Google Cloud Blob\nCreate a model instance using AutoMLWrapper\nImport our uploaded dataset\nTrain the AutoML model\nUse the model api for making predictions\n\nAnd that’s it!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the Synthetic AI Agents Blog about machine learning and agent based modelling. Watch your head, the ceiling for joke quality is quite low here!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Blog about Artificial Intelligence",
    "section": "",
    "text": "How to win a Kaggle NLP Competition\n\n\n\n\n\n\ncode\n\n\nfastai\n\n\nNLP\n\n\nkaggle\n\n\n\n\n\n\n\n\n\nSep 1, 2024\n\n\nAgentRichi\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to MATSim\n\n\n\n\n\n\ncode\n\n\nsoftware\n\n\n\n\n\n\n\n\n\nAug 23, 2024\n\n\nAgentZizi\n\n\n\n\n\n\n\n\n\n\n\n\nWho Directed That?\n\n\n\n\n\n\ncode\n\n\ndeep learning\n\n\nfastai\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nAgentRichi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/MATSim/MATSim.html",
    "href": "posts/MATSim/MATSim.html",
    "title": "Intro to MATSim",
    "section": "",
    "text": "What do you know about MATSim?\n\nIntro\nhenlo frienii"
  }
]